{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8befda82-99a7-4ea0-aea3-922a7f19cb06",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신 (SVM, Support Vector Machine)\n",
    "- 기존의 분류 방법들은 '오류율 최소화'의 목적으로 설계되었다면, SVM은 두 부류 사이에 존재하는 '여백 최대화'의 목적으로 설계되었다.\n",
    "- 분류 문제를 해결하는 지도 학습 모델 중 하나이며, 결정 경계라는 데이터 간 경계를 정의함으로써 분류를 할 수 있다.\n",
    "- 새로운 데이터가 경계를 기준으로 어떤 방향에 잡히는지를 확인함으로써 해당 데이터의 카테고리를 예측할 수 있다.\n",
    "- 데이터가 어느 카테고리에 속할지 판단하기 위해 가장 적절한 경계인 결정 경계를 찾는 선형 모델이다.  \n",
    "\n",
    "<img src=\"./images/support_vector_machine01.png\" width=\"400px\" style=\"margin-bottom: 60px;\">\n",
    "\n",
    "#### 서포트 벡터 (Support Vector)  \n",
    "\n",
    "- 결정 경계를 결정하는 데이터(벡터)등을 서포터 벡터라고 부른다.\n",
    "- 서포트 벡터들이 결정경계(Decision Boundary)를 결정한다.\n",
    "- 서포트 벡터와 결정 경계간의 거리를 마진(Margin)이라고 부르고, 마진이 크면 클 수록, 좋은 결정 경계가 된다.\n",
    "- 서포트 벡터들을 통해 결정 경계를 결정 하게 되고, 다른 학습 데이터들은 무시될 수 있기 때문에 SVM의 속도가 빠를 수 있다.\n",
    "\n",
    "#### 결정 경계 (Decision Boundary)\n",
    "- 새로운 데이터가 들어오더라도 결정 경계를 중심으로 두 집단이 멀리 떨어져 있어야 두 집단을 잘 구분할 수 있기 때문에 일반화 하기 쉬워진다.\n",
    "- 독립 변수의 차원 보다 한 차원 낮아지며, N차원 공간에서 한 차원 낮은 N-1 차원의 결정 경계가 생긴다.\n",
    "  즉 2차원 공간에서는 초평면이 선으로 결정되고, 고차원에서는 결정 경계는 선이 아닌 평면 이상의 도형이며, 이를 \"초평면(Hyperplane)\" 이라고 부른다.\n",
    "<sub>선을 왜 긋는가 -> 더 잘 분류하기 위해서</sub>\n",
    "<img src=\"./images/support_vector_machine02.png\" width=\"400px\">\n",
    "\n",
    "#### 하드 마진(Hard margin)\n",
    "- 매우 엄격하게 집단을 구분하는 방법으로 이상치를 허용해주지 않는 방법이다.\n",
    "- 이상치를 허용하지 않기 때문에 과적합이 발생하기 쉽고, 최적의 결정 경계를 잘못 구분하거나 못찾는 경우가 생길 수 있다.\n",
    "- C(cost)는 패널티를 조절 할 수 있고, 값이 커질 수록 결정 경계가 데이터에 더 정확하게 맞춰진다.\n",
    "- C를 낮추면 일을 덜 하게 하는 것이고, C를 높이면 일을 더해서 더 섬세하게 찾아낸다.\n",
    "- C가 너무 낮으면 underfitting될 가능성이 커지고, C가 너무 높으면 overfitting 이 발생할 수 있다.\n",
    "\n",
    "<img src=\"./images/hard_margin.png\" width=\"350px\" style=\"margin-bottom: 60px\">\n",
    "\n",
    "#### 소프트 마진(Soft margin)\n",
    "- 이상치를 허용해서 일부 데이터를 잘못 분류하더라도 나머지 데이터를 더욱 잘 분류해주는 방법이다.\n",
    "- 이상치 허용으로 인해 데이터의 패턴을 잘 감지하지 못하는 문제점이 생길 수 있다.\n",
    "\n",
    "<img src=\"./images/soft_margin.png\" width=\"550px\" style=\"margin-bottom: 60px\">\n",
    "\n",
    "> 📌 정리\n",
    "> \n",
    "> 서포트 벡터 머신 알고리즘을 적용한 SVC 모델의 하이퍼파라미터는 Regularization cost, C에 값을 전달하여 E(패널티)를 조절할 수 있다.\n",
    "> \n",
    "> C가 클 수록 loss function에서 오차항인 EPSILON<sub>i</sub>의 영향력이 커지게 되기 때문에, 마진의 크기가 줄어들고(하드마진) , 반대로 C가 작을 수록 마진의 크기가 늘어난다.(Soft Margin). 적절히 조절하면 오히려 성능이 좋아질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e410f-4070-46c6-899e-718ecb50e238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
